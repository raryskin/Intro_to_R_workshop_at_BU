---
title: "Intro to R at Aphasia Research Lab Meeting"
author: 'Rachel Ryskin '
date: "11/16/2017"
output:
  html_document: default

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

In this tutorial, I'll try to cover some of the basic ways of interacting with R and then pretty quickly switch to data wrangling and data visualization.  I recommend usign RStudio, <https://support.rstudio.com>, because they have lots of great resources
 (e.g., Help --> Cheatsheets)

Another great resource for many of the functions we'll use: http://r4ds.had.co.nz/

First of all, R can be used as a calculator:

```{r}
1+1
2*3
3^2
abs(-2)
log(1)
sqrt(25)
5 %% 3
```

It can also handle text:

```{r}
print("hello world")
```

It's a good idea not to work directly in the console but to have a script where you will first write your commands and then excute them in the console. Once you open a new R script there are a few useful things to note:
+ Use # to comment lines so they don't get executed
+ You can send a line directly to console from script with command + return (on mac) or ctrl + enter (windows)
+ I like to put `rm(list = ls())` at the beginning of all my scripts so that I can be sure there are no leftover variables.

# Variables

Values can be stored as variables in a few different ways

```{r}
a = 2
b = a + 3
b
```

This can also be done with arrows

```{r}
a<-2
a+3->b
b
c<-b^2
c
```

When you're defining variables, try to use meaningful names like `average_of_ratings` rather than `variable1`

# Vectors

If you want to store more than one thing in a variable you may want to make a vector.To do this you will use the function `c()` which combines values

```{r}
v1 = c(1,2,3,4,5,6,7,8,9,10)
v1
```

HINT: If you are ever unsure about how a function works or what arguments it takes. Typing `?<FUNCTION NAME GOES HERE>` will open up a help file (e.g., `?c`).

Now that we have some values stored in a vector, we may want to access those values and we can do this by using their position in the vector or **index**. 

```{r}
v1[1]
v1[5]
v1[-1]
v1[c(2,7)]
v1[-c(2,7)]
v1[1:3]
v1[-(1:3)]
```

You may also want to get some overall information about the vector:

+ what types of values are in `v1`?

```{r}
str(v1)
summary(v1)
```

+ how long is `v1`?

```{r}
length(v1)
```

+ what is the average of all these values?

```{r}
mean(v1)
```

+ what is the standard deviation of all these values?

```{r}
sd(v1)
```

Earlier we created `v1` by simply listing all the elements in it and using`c()` but if you have lots of values, this is very tedious. There are some functions that can help make vectors more efficiently

```{r}
v2=(1:10)
v2
v3=rep(x = 1, times = 10)
v3
v4=rep(1:2,5)
v4
v5=seq(from = 1, to = 20, by = 2) 
v5

```

Note that for v4, I didn't include the names of the arguments but R figures out which is which by the order

You can also apply operations to all elements of the vector simultaneously.

```{r}

 v1 + 1

 v1 * 100

```

You can also do pair-wise operations on 2 vectors.

```{r}
v1 + v2
```

## Character Vectors

So far we've looked at **numeric** variables and vectors, but they can als be **strings**

```{r}
name = 'Rachel'
name

friends = c('Rachel', 'Ross', 'Joey', 'Monica', 'Chandler', 'Phoebe')
friends

str(friends)
```

You can even store numbers as strings (and sometimes data you load from a file will be stored this way so watch out for that)

```{r}

some_numbers = c('2', '3', '4')
```

...but you can't manipulate them as numbers

```{r,eval=FALSE}

some_numbers +1
```

So you might want to convert the strings into numbers first using `as.numeric()`

```{r}
some_numbers = as.numeric(some_numbers)

some_numbers +1
```


## NA

Another important datatype is NA. Say I'm storing people's heights in inches in a dataframe, but I don't have data on the third person.

```{r}

heights = c(72, 70, NA, 64)
str(heights)

```

Even though it's composed of letters NA is not a string, in this case it's numeric, and represents a missing value, or an invalid value, or whatever. You can still perform operations on the height vector:

```{r}
heights + 1
heights * 2
```

if you had an NA in a vector of strings, its datatype would be a character.

```{r}
friends = c('Rachel', NA, 'Joey', 'Monica', 'Chandler', 'Phoebe')

str(friends)
```

If you have NA in your vector and want to use a function on it, this can complicate things

```{r}
mean(heights)
```

To avoid returning NA, you may want to just throw out the NA values using `na.omit` and work with what's left.

```{r}

heights_no_NA = na.omit(heights)
mean(heights_no_NA)
```

Alternatively, many functions have a built-in argument `na.rm` that you can use to tell the function what to do about NA values. So you can do the previous step in 1 line:

```{r}

mean(heights_no_NA, na.rm=TRUE)
```
It can also be useful to know if a vector contains NA ahead of time and where those values are:

```{r}

is.na(heights)
which(is.na(heights))

```

## Booleans

This brings us to another important datatype: booleans. They are `TRUE` or `FALSE`, or `T` or `F`. Here are some expressions that return boolean values:

```{r}
1 < 100
500 == 500 ## for equality testing, use double-equals!
1 == 2 | 2 == 2 ## OR
1 == 1 & 100 == 100 ## AND
1 == 1 & 100 == 101 ## AND

```

## Try it yourself...

1. Make a vector, 'tens' of all the multiples of 10 up to 200.

2. Find the indeces of the numbers divisible by 3 

```{r}

tens = seq(from=10,to=200,by=10)
tens
which(tens %% 3 == 0)
```


# Dataframes

Most data you will work with in R will be in a dataframe format. Dataframes are basically tables where each column is a vector with a name. Dataframes can combine character vectors, numeric vectors, logical (boolean) vectors, etc. This is really useful for data from experiments where you may want one column to contain information about the name of the condition (a string) and another column to contain response times (a number).

Let's read in some data!

**But first a digression...**
One of the best things about R is that it is open-source and lots of R users who find that some functionality is missing from base R (which is what we've been using so far) will write their own functions and then share them with the R community. Often times they'll write whole **packages** of functions to greatly enhance the capabilities of base R. In order for you to use those packages, they need to be installed on your computer and loaded up in your current session. For current purposes, you will need the `tidyverse` package and you can install it with this simple command:

```{r, eval=FALSE}
install.packages("tidyverse")

```

When the installation is done, load up the library of functions in the package with the following command:

```{r}
library(tidyverse)
```

Okay, digression over.

Let's read in your lexical decision data from earlier using a `tidyverse` function called `read_csv()`. 

```{r}
typicality_training_data <- read_csv("R_workshop_typicality_training_data.csv")
```

```{r}
subject_data <- read_csv("R_workshop_subject_data.csv")
```

Our data is now stored as a dataframe. The output message tells us what datatype `read_csv()` assigned to every column. It usually does a pretty good job of guessing the appropriate datatype but on occasion you may have to correct using a function like `as.numeric()` or `as.character()`.

Note that the path to the data can be any folder on your computer or online (a url). If you just put in the filename without the path, it will look for the file in the local folder.

Also note, that if you have column headers in your csv file, `read_csv()` will automatically name your columns accordingly and you won't have to specify `col_names=`.

At this point, it's a good idea to look at your data to make sure everything was correctly uploaded. In R Studio, you can open up a viewing pane with the command `View(typicality_training_data)` to see the data in spreadsheet form. You can also use `summary()`, `str()` and `glimpse()`

```{r}
glimpse(typicality_training_data)
```

You can also extract just the names of the columns:

```{r}
names(typicality_training_data)
```

Or just the first (or last) few rows of the dataframe:

```{r}

head(typicality_training_data)
tail(typicality_training_data)

```

Or look at the dimensions of your dataframe

```{r}
dim(typicality_training_data)
nrow(typicality_training_data)
ncol(typicality_training_data)
```

To access a specific column, row, or cell you can use **indexing** in much the same way you can with vectors (just now with 2 dimensions)

```{r}
typicality_training_data[1,2] # what's in the 1st row, 2nd column
typicality_training_data[1,] # the 1st row for all columns
typicality_training_data[,2] # all rows for the 2nd column
typicality_training_data[,c(2,5)] # all rows for columns 2 and 5
typicality_training_data[,c('Patient','Accuracy')]
```

Another easy way to extract a dataframe column is by using the `$` operator and the column name

```{r}
head(typicality_training_data$Accuracy)
```

 `typicality_training_data$Accuracy` is a (numeric) vector so you can perform various operations on it (as we saw earlier)

```{r}
head(typicality_training_data$Accuracy *2)

mean(typicality_training_data$Accuracy,na.rm=TRUE)
```


# Data Wrangling (tidyr/dplyr)

Often when you upload data it's not yet in a convenient, "tidy" form so data wrangling refers to the various cleaning and re-arranging steps between uploading data and being able to visualize or analyze it. I'll start out by showing you a few of the most common things you might want to do with your data. 

For example, you may want to combine the data from the typicality training task with measures of each subject's individual difference mesures such as WAB AQ scores.

## Join

```{r}
all_data = left_join(typicality_training_data,subject_data, by = c('Patient' = 'SubjectID'))
```


## Mutate

Now let's say we don't want just Pre and Post but also Pre-Post; we can add a column to our dataframe using the `mutate()` function.

```{r}
all_data = mutate(all_data, WABchange = `WAB AQ Post`-`WAB AQ Pre` )
head(all_data)
```

`mutate()` is extremely useful anytime you want to add information to your data. For instance, Maybe we want to code for who improves on the WAB and first half of treatment versus second half and create a composite score. This can be done all at once.

```{r}
head(all_data<-mutate(all_data,
               WABimprovement = WABchange > 0,
               half_of_training = if_else(Session <=8, 'first','second'))
)
           
```

Note:
+ `if_else()` is a useful function which takes a logical comparison as a first argument and then what to do if it is `TRUE` as the 2nd argument and what to do if it is `FALSE` as the 3rd.

## Filter

Now let's say we want to look at only a subset of the data, we can `filter()` it:

```{r}

all_data_improved = filter(all_data,WABimprovement == TRUE)

```

## Select

If your dataframe is getting unruly, you can focus on a few key columns with `select()`

```{r}

critical_data<-select(all_data,Patient,Accuracy,Session)

```


## Arrange

You can also sort the dataframe by one of the columns:

```{r}

head(arrange(all_data,WABchange))

```


## Group_by and Summarize

Most of the time when you have data, the ultimate goal is to summarize it in some way. For example, you may want to know the mean Accuracy for each subject by type of item.

```{r}

summarize(group_by(all_data,Typicality,Trained),mean_accuracy=mean(Accuracy,na.rm=TRUE))

```

As you can see, we often want to string `tidyverse` functions together which can get difficult to read. The solution to this is...

## Piping ` %>% `

We can create a pipeline where the dataframe undergoes various transformations one after the other with the same functions, `mutate()`, `filter()`, etc. without having to repeat the name of the dataframe over and over and much more intuitive syntax.

```{r}

# this is the previous syntax
head(all_data<-mutate(all_data,
                      WABimprovement = WABchange > 0,
                      half_of_training = if_else(Session <=8, 'first','second'))
     )

# this is the piping syntax
all_data_extra = all_data %>% mutate(
  WABimprovement = WABchange > 0,
  half_of_training = if_else(Session <=8, 'first','second'))
```


And we can keep adding functions to the pipeline very easily...

```{r}

all_data_extra = all_data %>% mutate(
  WABimprovement = WABchange > 0,
  half_of_training = if_else(Session <=8, 'first','second')) %>% 
  filter(`WAB AQ Pre` > 50) %>% 
  group_by(Typicality,Trained) %>% 
  summarize(mean_accuracy=mean(Accuracy,na.rm=TRUE))
all_data_extra
```



We can look just at conditions and add some summary stats

```{r}
all_data_summary  = all_data %>% 
  group_by(Session,Typicality,Trained) %>% 
  summarise(
    mean_acc = mean(Accuracy,na.rm=TRUE),
    median_acc = median(Accuracy,na.rm=TRUE),
    sd_acc = sd(Accuracy,na.rm=TRUE),
    counts = n()
  ) 
all_data_summary
```



## Try it yourself

1. Add a new column to `all_data` called `over_60` that codes whether the patient was over 60 or not

2. Get the means and counts for Accuracy by Typicality and Trained Category and by Age group

```{r}

all_data_summary2  = all_data %>% 
  mutate(over_60 = Age > 60) %>% 
  group_by(over_60,Typicality,Trained) %>% 
  summarise(
    mean_acc = mean(Accuracy,na.rm=TRUE),
    counts = n()
  ) 
all_data_summary2

```


# Data Visualization (ggplot2)

 Looking at columns of numbers isn't really the best way to do data analysis. You could be tripped up by placement of decimal points, you might accidentally miss a big number. It would be much better if we could PLOT these numbers so we can visually tell if anything stands out.

If you've taken an introductory Psych or Neuro course you might know that a huge proportion of human cortex is devoted to visual information processing; we have hugely powerful abilities to process visual data. By using plotting we can leverage that ability to get a fast sense of what is going on in our data.

Visualizing your data is an extremely important part of any data analysis. `tidyverse` contains a whole library of functions for plotting: `ggplot2`. I'll be showing you how to use these functions but also I'll be trying to give you some intuitions about how researchers use visualization to get a better understanding of their data.

The first thing you might want to know is what your dependent variable, in this case the Accuracy, looks like. In other words, how is it distributed?

## Histograms
```{r}

ggplot(data=all_data)+ geom_histogram(mapping = aes(x=Accuracy))

```

`ggplot` syntax might seem a little unusual. You can think of it as first creating a plot coordinate system with `ggplot()` and the you can add layers of information with `+`. 

`ggplot(data=all_data)` would create an empty plot because you haven't told it anything about what variables you're interested in or how you want to look at them. This is where geometric objects, or "geoms", come in. 

`geom_histogram()` is going to make this plot a histogram. A histogram has values of whatever variable you choose on the x axis and counts of those values on the y. The aesthetic mapping, `aes()`, arguments let us specify which variable, in this case `rt`, we want to know the distribution of. We can also change visual aspects of the geom, like the width of the bins, depending on what will make the graph more clear and informative.

```{r}

ggplot(data=all_data)+ geom_histogram(mapping = aes(x=Accuracy), bins=25)

```

## Bar graphs

Contrary to `geom_histogram()` this takes a minimum of 2 arguments, x and y.

```{r}
all_data_means  = all_data %>% 
  group_by(Typicality,Trained) %>% 
  summarise(
    mean_acc = mean(Accuracy,na.rm=TRUE),
    median_acc = median(Accuracy,na.rm=TRUE),
    sd_acc = sd(Accuracy,na.rm=TRUE),
    counts = n()
  ) 


ggplot(all_data_means) + geom_col(aes(x=Trained, y=mean_acc,fill = Typicality), position='dodge' )
```

## Violin plots

```{r}
ggplot(all_data) + geom_violin(aes(x=Trained, y=Accuracy,fill = Typicality) )
```

## Box plots

```{r}
ggplot(all_data) + geom_boxplot(aes(x=Trained, y=Accuracy,fill = Typicality) )
```

## Scatterplots

Let's say we are curious to see if people get more accurate over the course of the sessions. So, we want to plot session number and compare it with mean Accuracy. Let's put session number on the x axis, mean Accuracy on the y axis, and make a scatterplot. For this we're going to use a different geom, `geom_point()`. Contrary to `geom_histogram()` this takes a minimum of 2 arguments, x and y.

```{r}
ggplot(all_data) + geom_point(aes(x=Session, y=Accuracy))
```

This is a fair number of data points so it's a little difficult to see what's going on. It might be useful to also show what the average across participants looks like at every timepoint. We can just add another layer to this same graph with the `+`, in this case we'll use `geom_line()`.


```{r}
all_data_by_session = all_data %>%
  group_by(Session) %>%
  summarise(mean_acc_by_sess=mean(Accuracy,na.rm = TRUE))

ggplot() + geom_point(data = all_data, aes(x=Session, y=Accuracy),alpha=0.2) +
  geom_line(data = all_data_by_session ,aes(x=Session, y=mean_acc_by_sess),color='blue')
```

+ Because there were so many points and it was difficult to see, I made each point less opaque using `alpha=0.2` as an argument for `geom_point()` and I made the line connecting averages stand out by making it blue with `color='blue'` as an argumen to `geom_line()`

+ Note that I'm plotting 2 different datasets withing the same graph and this is easy to do because you can define data separately for each geom.

```{r}
all_data_by_session = all_data %>%
  group_by(Session) %>%
  summarise(mean_acc_by_sess=mean(Accuracy,na.rm = TRUE))

ggplot() + geom_point(data = all_data, aes(x=Session, y=Accuracy, color = Typicality, shape = Trained),alpha=0.2) +
  geom_smooth(data = all_data ,aes(x=Session, y=Accuracy, color = Typicality, linetype = Trained),method = 'lm')
```
```{r}
all_data_means = all_data %>%
  group_by(Session,Typicality,Trained) %>%
  summarise(mean_acc=mean(Accuracy,na.rm = TRUE))

ggplot() + geom_point(data = all_data_means, aes(x=Session, y=mean_acc, color = Typicality, shape = Trained),size = 3) +
  geom_smooth(data = all_data ,aes(x=Session, y=Accuracy, color = Typicality, linetype = Trained),method = 'lm',se=FALSE)
```

```{r}
ggplot() + geom_jitter(data = all_data, aes(x=Session, y=Accuracy, color = Typicality, shape = Trained),alpha=0.2) +
  geom_smooth(data = all_data ,aes(x=Session, y=Accuracy, color = Typicality, linetype = Trained),method = 'lm', se = FALSE)
```

```{r}
ggplot() + geom_jitter(data = all_data, aes(x=Session, y=Accuracy, color = Typicality, shape = Trained),alpha=0.2) +
  geom_smooth(data = all_data ,aes(x=Session, y=Accuracy, color = Typicality, linetype = Trained),method = 'lm', se = FALSE) +
  theme_minimal()
```

```{r}
ggplot() +
  geom_smooth(data = all_data ,aes(x=Session, y=Accuracy, color = Typicality, linetype = Trained),method = 'lm', se = FALSE) +
  coord_cartesian(ylim = c(0,1))+
  scale_color_discrete(limits= c('A','T'),labels = c('Atypical','Typical'))+
  scale_linetype_discrete(limits= c('M','T','U'),labels = c('Monitored','Trained', 'Untrained'))+
  theme_classic()+
  theme(text = element_text(size = 20))
```

```{r}
all_data_means = all_data %>%
  group_by(Session,Typicality,Trained) %>%
  summarise(mean_acc=mean(Accuracy,na.rm = TRUE))

ggplot() + geom_point(data = all_data_means, aes(x=Session, y=mean_acc, color = Typicality, shape = Trained),alpha = 0.5,size = 3) +
  geom_smooth(data = all_data ,aes(x=Session, y=Accuracy, color = Typicality, linetype = Trained),method = 'lm', se = FALSE) +
  coord_cartesian(ylim = c(0,1))+
  scale_color_discrete(limits= c('A','T'),labels = c('Atypical','Typical'))+
  scale_linetype_discrete(limits= c('M','T','U'),labels = c('Monitored','Trained', 'Untrained'))+
  scale_shape_discrete(limits= c('M','T','U'),labels = c('Monitored','Trained', 'Untrained'))+
  theme_classic()+
  theme(text = element_text(size = 20))
```


## Facets

Now maybe we want to split the data based on AQ and plot the data side by side. `ggplot` has an easy way to do that with __facetting__.

```{r}
all_data_improvement = all_data %>%
  mutate(improved = if_else((`WAB AQ Post`-`WAB AQ Pre`) >0 , 'improved','not improved')) 

ggplot() +
  geom_smooth(data = all_data_improvement ,aes(x=Session, y=Accuracy, color = Typicality, linetype = Trained),method = 'lm', se = FALSE) +
  facet_wrap(~improved)+
  coord_cartesian(ylim = c(0,1))+
  scale_color_discrete(limits= c('A','T'),labels = c('Atypical','Typical'))+
  scale_linetype_discrete(limits= c('M','T','U'),labels = c('Monitored','Trained', 'Untrained'))+
  theme_classic()+
  theme(text = element_text(size = 20))
```

`facet_wrap()` includes a `~` followed by the variable you want to split your data by. Facetting works with any geom and is a great way to de-clutter a plot.

## Try it yourself

1. Read in the following file: http://web.mit.edu/ryskin/www/four_datasets.csv

It contains 4 datasets (A,B,C,D) and x and y values for each.

2. For each dataset, get:

+ the mean of x
+ the mean of y
+ the standard deviation of x 
+ the standard deviation of y
+ the correlation of x and y (using `cor()`)

3. Using `facet_wrap()` make a figure with 4 scatterplots, one for each dataset.

What do you observe?

```{r}
datasets = read_csv('http://web.mit.edu/ryskin/www/four_datasets.csv')

datasets_summ = datasets %>% 
  group_by(dataset) %>% 
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    stdev_x = sd(x),
    stdev_y = sd(y),
    corr_xy = cor(x,y)
  )

datasets_summ

ggplot(datasets) + geom_point(aes(x=x,y=y))+facet_wrap(~dataset)

ggplot(datasets,aes(x=x,y=y)) + geom_point()+geom_smooth(method=lm)+facet_wrap(~dataset)

```

